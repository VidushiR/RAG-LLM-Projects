{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20831971-b4ae-4a1b-a088-038f687526f2",
   "metadata": {},
   "source": [
    "# Data Preparation for RAG (AskMyDocs)\n",
    "\n",
    "## Goal\n",
    "Convert arXiv metadata into a clean, filtered, chunked corpus suitable for Retrieval-Augmented Generation (RAG).\n",
    "\n",
    "### Outputs\n",
    "1. Clean document table (1 row per paper)\n",
    "2. Chunked corpus table (many rows per paper)\n",
    "\n",
    "### What this notebook does\n",
    "- Loads raw arXiv metadata\n",
    "- Cleans text fields (title, abstract)\n",
    "- Parses categories into list form\n",
    "- Filters dataset to target categories\n",
    "- Builds a canonical `text` field for embedding\n",
    "- Chunks text into retrieval-friendly segments\n",
    "- Saves processed datasets to `data/processed/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "597ff112-0fc0-403e-8ef9-e012a311fa9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import modules\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 120)\n",
    "pd.set_option(\"display.max_columns\", 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "443c7c92-b7e9-41cc-ad36-5077450bb954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Project paths (adjust if your repo differs) ---\n",
    "PROJECT_ROOT = Path(\"..\")  # if notebook is in /notebooks\n",
    "RAW_DIR = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "PROCESSED_DIR = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Input file ---\n",
    "# Update this to your actual raw file name\n",
    "RAW_FILE = RAW_DIR / \"arxiv-metadata-oai-snapshot.json\"\n",
    "\n",
    "# --- Target categories (from your EDA) ---\n",
    "TARGET_CATS = {\"cs.CL\", \"cs.LG\", \"cs.AI\"}\n",
    "\n",
    "# --- Basic filtering thresholds ---\n",
    "MIN_ABSTRACT_CHARS = 200  # tighten/loosen as needed\n",
    "\n",
    "# --- Chunking config ---\n",
    "CHUNK_SIZE_CHARS = 1200   # simple, fast baseline\n",
    "CHUNK_OVERLAP_CHARS = 200\n",
    "\n",
    "# --- Output names ---\n",
    "RUN_TAG = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "DOCS_OUT = PROCESSED_DIR / f\"arxiv_docs_clean_{RUN_TAG}.csv\"\n",
    "CHUNKS_OUT = PROCESSED_DIR / f\"arxiv_chunks_{RUN_TAG}.csv\"\n",
    "SCHEMA_OUT = PROCESSED_DIR / f\"schema_{RUN_TAG}.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f4c23e5-3350-40a9-b0df-4a607c513b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\vidus\\Projects\\RAG-LLM-Projects\\AskMyDocs-RAG-Chatbot\\notebooks\n",
      "RAW_DIR exists? True\n",
      "RAW_FILE exists? True\n",
      "\n",
      "Files inside raw folder:\n",
      " - .gitkeep\n",
      " - arxiv-metadata-oai-snapshot.json\n"
     ]
    }
   ],
   "source": [
    "print(\"Current working directory:\", Path.cwd())\n",
    "print(\"RAW_DIR exists?\", RAW_DIR.exists())\n",
    "print(\"RAW_FILE exists?\", RAW_FILE.exists())\n",
    "\n",
    "if RAW_DIR.exists():\n",
    "    print(\"\\nFiles inside raw folder:\")\n",
    "    for f in RAW_DIR.iterdir():\n",
    "        print(\" -\", f.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f08e6180-7fca-4b68-adce-626c1ab9356a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (10000, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>submitter</th>\n",
       "      <th>authors</th>\n",
       "      <th>title</th>\n",
       "      <th>comments</th>\n",
       "      <th>journal-ref</th>\n",
       "      <th>doi</th>\n",
       "      <th>report-no</th>\n",
       "      <th>categories</th>\n",
       "      <th>license</th>\n",
       "      <th>abstract</th>\n",
       "      <th>versions</th>\n",
       "      <th>update_date</th>\n",
       "      <th>authors_parsed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>704.0001</td>\n",
       "      <td>Pavel Nadolsky</td>\n",
       "      <td>C. Bal\\'azs, E. L. Berger, P. M. Nadolsky, C.-P. Yuan</td>\n",
       "      <td>Calculation of prompt diphoton production cross sections at Tevatron and\\n  LHC energies</td>\n",
       "      <td>37 pages, 15 figures; published version</td>\n",
       "      <td>Phys.Rev.D76:013009,2007</td>\n",
       "      <td>10.1103/PhysRevD.76.013009</td>\n",
       "      <td>ANL-HEP-PR-07-12</td>\n",
       "      <td>hep-ph</td>\n",
       "      <td>None</td>\n",
       "      <td>A fully differential calculation in perturbative quantum chromodynamics is\\npresented for the production of massiv...</td>\n",
       "      <td>[{'version': 'v1', 'created': 'Mon, 2 Apr 2007 19:18:42 GMT'}, {'version': 'v2', 'created': 'Tue, 24 Jul 2007 20:10:...</td>\n",
       "      <td>2008-11-26</td>\n",
       "      <td>[[Balázs, C., ], [Berger, E. L., ], [Nadolsky, P. M., ], [Yuan, C. -P., ]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>704.0002</td>\n",
       "      <td>Louis Theran</td>\n",
       "      <td>Ileana Streinu and Louis Theran</td>\n",
       "      <td>Sparsity-certifying Graph Decompositions</td>\n",
       "      <td>To appear in Graphs and Combinatorics</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>math.CO cs.CG</td>\n",
       "      <td>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</td>\n",
       "      <td>We describe a new algorithm, the $(k,\\ell)$-pebble game with colors, and use\\nit obtain a characterization of the ...</td>\n",
       "      <td>[{'version': 'v1', 'created': 'Sat, 31 Mar 2007 02:26:18 GMT'}, {'version': 'v2', 'created': 'Sat, 13 Dec 2008 17:26...</td>\n",
       "      <td>2008-12-13</td>\n",
       "      <td>[[Streinu, Ileana, ], [Theran, Louis, ]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id       submitter  \\\n",
       "0  704.0001  Pavel Nadolsky   \n",
       "1  704.0002    Louis Theran   \n",
       "\n",
       "                                                 authors  \\\n",
       "0  C. Bal\\'azs, E. L. Berger, P. M. Nadolsky, C.-P. Yuan   \n",
       "1                        Ileana Streinu and Louis Theran   \n",
       "\n",
       "                                                                                      title  \\\n",
       "0  Calculation of prompt diphoton production cross sections at Tevatron and\\n  LHC energies   \n",
       "1                                                  Sparsity-certifying Graph Decompositions   \n",
       "\n",
       "                                  comments               journal-ref  \\\n",
       "0  37 pages, 15 figures; published version  Phys.Rev.D76:013009,2007   \n",
       "1    To appear in Graphs and Combinatorics                      None   \n",
       "\n",
       "                          doi         report-no     categories  \\\n",
       "0  10.1103/PhysRevD.76.013009  ANL-HEP-PR-07-12         hep-ph   \n",
       "1                        None              None  math.CO cs.CG   \n",
       "\n",
       "                                               license  \\\n",
       "0                                                 None   \n",
       "1  http://arxiv.org/licenses/nonexclusive-distrib/1.0/   \n",
       "\n",
       "                                                                                                                  abstract  \\\n",
       "0    A fully differential calculation in perturbative quantum chromodynamics is\\npresented for the production of massiv...   \n",
       "1    We describe a new algorithm, the $(k,\\ell)$-pebble game with colors, and use\\nit obtain a characterization of the ...   \n",
       "\n",
       "                                                                                                                  versions  \\\n",
       "0  [{'version': 'v1', 'created': 'Mon, 2 Apr 2007 19:18:42 GMT'}, {'version': 'v2', 'created': 'Tue, 24 Jul 2007 20:10:...   \n",
       "1  [{'version': 'v1', 'created': 'Sat, 31 Mar 2007 02:26:18 GMT'}, {'version': 'v2', 'created': 'Sat, 13 Dec 2008 17:26...   \n",
       "\n",
       "  update_date  \\\n",
       "0  2008-11-26   \n",
       "1  2008-12-13   \n",
       "\n",
       "                                                               authors_parsed  \n",
       "0  [[Balázs, C., ], [Berger, E. L., ], [Nadolsky, P. M., ], [Yuan, C. -P., ]]  \n",
       "1                                    [[Streinu, Ileana, ], [Theran, Louis, ]]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Columns: ['id', 'submitter', 'authors', 'title', 'comments', 'journal-ref', 'doi', 'report-no', 'categories', 'license', 'abstract', 'versions', 'update_date', 'authors_parsed']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load a sample first (full file is huge)\n",
    "df = pd.read_json(RAW_FILE, lines=True, nrows=10000)\n",
    "\n",
    "print(\"Shape:\", df.shape)\n",
    "display(df.head(2))\n",
    "print(\"\\nColumns:\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d00f9a1-7a58-44e7-91ce-48c1adc6dd12",
   "metadata": {},
   "source": [
    "## Clean + Select Only What is nedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44aba9cb-0e54-48e8-8432-3b86b741f5ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>categories</th>\n",
       "      <th>update_date</th>\n",
       "      <th>authors</th>\n",
       "      <th>authors_parsed</th>\n",
       "      <th>title_clean</th>\n",
       "      <th>abstract_clean</th>\n",
       "      <th>abstract_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>704.0001</td>\n",
       "      <td>Calculation of prompt diphoton production cross sections at Tevatron and\\n  LHC energies</td>\n",
       "      <td>A fully differential calculation in perturbative quantum chromodynamics is\\npresented for the production of massiv...</td>\n",
       "      <td>hep-ph</td>\n",
       "      <td>2008-11-26</td>\n",
       "      <td>C. Bal\\'azs, E. L. Berger, P. M. Nadolsky, C.-P. Yuan</td>\n",
       "      <td>[[Balázs, C., ], [Berger, E. L., ], [Nadolsky, P. M., ], [Yuan, C. -P., ]]</td>\n",
       "      <td>Calculation of prompt diphoton production cross sections at Tevatron and LHC energies</td>\n",
       "      <td>A fully differential calculation in perturbative quantum chromodynamics is presented for the production of massive p...</td>\n",
       "      <td>980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>704.0002</td>\n",
       "      <td>Sparsity-certifying Graph Decompositions</td>\n",
       "      <td>We describe a new algorithm, the $(k,\\ell)$-pebble game with colors, and use\\nit obtain a characterization of the ...</td>\n",
       "      <td>math.CO cs.CG</td>\n",
       "      <td>2008-12-13</td>\n",
       "      <td>Ileana Streinu and Louis Theran</td>\n",
       "      <td>[[Streinu, Ileana, ], [Theran, Louis, ]]</td>\n",
       "      <td>Sparsity-certifying Graph Decompositions</td>\n",
       "      <td>We describe a new algorithm, the $(k,\\ell)$-pebble game with colors, and use it obtain a characterization of the fam...</td>\n",
       "      <td>795</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  \\\n",
       "0  704.0001   \n",
       "1  704.0002   \n",
       "\n",
       "                                                                                      title  \\\n",
       "0  Calculation of prompt diphoton production cross sections at Tevatron and\\n  LHC energies   \n",
       "1                                                  Sparsity-certifying Graph Decompositions   \n",
       "\n",
       "                                                                                                                  abstract  \\\n",
       "0    A fully differential calculation in perturbative quantum chromodynamics is\\npresented for the production of massiv...   \n",
       "1    We describe a new algorithm, the $(k,\\ell)$-pebble game with colors, and use\\nit obtain a characterization of the ...   \n",
       "\n",
       "      categories update_date  \\\n",
       "0         hep-ph  2008-11-26   \n",
       "1  math.CO cs.CG  2008-12-13   \n",
       "\n",
       "                                                 authors  \\\n",
       "0  C. Bal\\'azs, E. L. Berger, P. M. Nadolsky, C.-P. Yuan   \n",
       "1                        Ileana Streinu and Louis Theran   \n",
       "\n",
       "                                                               authors_parsed  \\\n",
       "0  [[Balázs, C., ], [Berger, E. L., ], [Nadolsky, P. M., ], [Yuan, C. -P., ]]   \n",
       "1                                    [[Streinu, Ileana, ], [Theran, Louis, ]]   \n",
       "\n",
       "                                                                             title_clean  \\\n",
       "0  Calculation of prompt diphoton production cross sections at Tevatron and LHC energies   \n",
       "1                                               Sparsity-certifying Graph Decompositions   \n",
       "\n",
       "                                                                                                            abstract_clean  \\\n",
       "0  A fully differential calculation in perturbative quantum chromodynamics is presented for the production of massive p...   \n",
       "1  We describe a new algorithm, the $(k,\\ell)$-pebble game with colors, and use it obtain a characterization of the fam...   \n",
       "\n",
       "   abstract_len  \n",
       "0           980  \n",
       "1           795  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "KEEP_COLS = [\n",
    "    \"id\", \"title\", \"abstract\", \"categories\",\n",
    "    \"update_date\", \"authors\", \"authors_parsed\"\n",
    "]\n",
    "\n",
    "df_base = df[KEEP_COLS].copy()\n",
    "\n",
    "_ws = re.compile(r\"\\s+\")\n",
    "_nl = re.compile(r\"(\\\\n|\\n|\\r)\")\n",
    "\n",
    "def clean_text(x):\n",
    "    if pd.isna(x):\n",
    "        return \"\"\n",
    "    x = str(x)\n",
    "    x = _nl.sub(\" \", x)\n",
    "    x = _ws.sub(\" \", x)\n",
    "    return x.strip()\n",
    "\n",
    "df_base[\"title_clean\"] = df_base[\"title\"].apply(clean_text)\n",
    "df_base[\"abstract_clean\"] = df_base[\"abstract\"].apply(clean_text)\n",
    "df_base[\"abstract_len\"] = df_base[\"abstract_clean\"].str.len()\n",
    "\n",
    "df_base.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e8aeae-7239-4b85-8472-73f1accd5990",
   "metadata": {},
   "source": [
    "### Filter to Target CS Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16ac8352-8196-4966-9be3-a84177094272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 10000\n",
      "Rows with target categories: 76\n",
      "Rows with abstract >= 200 chars: 9659\n",
      "Filtered df_rag shape: (76, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>categories</th>\n",
       "      <th>update_date</th>\n",
       "      <th>authors</th>\n",
       "      <th>authors_parsed</th>\n",
       "      <th>title_clean</th>\n",
       "      <th>abstract_clean</th>\n",
       "      <th>abstract_len</th>\n",
       "      <th>categories_list</th>\n",
       "      <th>has_target_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>704.0047</td>\n",
       "      <td>Intelligent location of simultaneously active acoustic emission sources:\\n  Part I</td>\n",
       "      <td>The intelligent acoustic emission locator is described in Part I, while Part\\nII discusses blind source separation...</td>\n",
       "      <td>cs.NE cs.AI</td>\n",
       "      <td>2009-09-29</td>\n",
       "      <td>T. Kosel and I. Grabec</td>\n",
       "      <td>[[Kosel, T., ], [Grabec, I., ]]</td>\n",
       "      <td>Intelligent location of simultaneously active acoustic emission sources: Part I</td>\n",
       "      <td>The intelligent acoustic emission locator is described in Part I, while Part II discusses blind source separation, t...</td>\n",
       "      <td>1170</td>\n",
       "      <td>[cs.NE, cs.AI]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>704.0050</td>\n",
       "      <td>Intelligent location of simultaneously active acoustic emission sources:\\n  Part II</td>\n",
       "      <td>Part I describes an intelligent acoustic emission locator, while Part II\\ndiscusses blind source separation, time ...</td>\n",
       "      <td>cs.NE cs.AI</td>\n",
       "      <td>2007-05-23</td>\n",
       "      <td>T. Kosel and I. Grabec</td>\n",
       "      <td>[[Kosel, T., ], [Grabec, I., ]]</td>\n",
       "      <td>Intelligent location of simultaneously active acoustic emission sources: Part II</td>\n",
       "      <td>Part I describes an intelligent acoustic emission locator, while Part II discusses blind source separation, time del...</td>\n",
       "      <td>948</td>\n",
       "      <td>[cs.NE, cs.AI]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  \\\n",
       "0  704.0047   \n",
       "1  704.0050   \n",
       "\n",
       "                                                                                 title  \\\n",
       "0   Intelligent location of simultaneously active acoustic emission sources:\\n  Part I   \n",
       "1  Intelligent location of simultaneously active acoustic emission sources:\\n  Part II   \n",
       "\n",
       "                                                                                                                  abstract  \\\n",
       "0    The intelligent acoustic emission locator is described in Part I, while Part\\nII discusses blind source separation...   \n",
       "1    Part I describes an intelligent acoustic emission locator, while Part II\\ndiscusses blind source separation, time ...   \n",
       "\n",
       "    categories update_date                 authors  \\\n",
       "0  cs.NE cs.AI  2009-09-29  T. Kosel and I. Grabec   \n",
       "1  cs.NE cs.AI  2007-05-23  T. Kosel and I. Grabec   \n",
       "\n",
       "                    authors_parsed  \\\n",
       "0  [[Kosel, T., ], [Grabec, I., ]]   \n",
       "1  [[Kosel, T., ], [Grabec, I., ]]   \n",
       "\n",
       "                                                                        title_clean  \\\n",
       "0   Intelligent location of simultaneously active acoustic emission sources: Part I   \n",
       "1  Intelligent location of simultaneously active acoustic emission sources: Part II   \n",
       "\n",
       "                                                                                                            abstract_clean  \\\n",
       "0  The intelligent acoustic emission locator is described in Part I, while Part II discusses blind source separation, t...   \n",
       "1  Part I describes an intelligent acoustic emission locator, while Part II discusses blind source separation, time del...   \n",
       "\n",
       "   abstract_len categories_list  has_target_cat  \n",
       "0          1170  [cs.NE, cs.AI]            True  \n",
       "1           948  [cs.NE, cs.AI]            True  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TARGET_CATS = {\"cs.CL\", \"cs.LG\", \"cs.AI\"}\n",
    "MIN_ABSTRACT_CHARS = 200\n",
    "\n",
    "def parse_categories(cat_str):\n",
    "    if pd.isna(cat_str):\n",
    "        return []\n",
    "    return cat_str.split(\" \")\n",
    "\n",
    "df_base[\"categories_list\"] = df_base[\"categories\"].apply(parse_categories)\n",
    "\n",
    "def has_target_cat(cat_list):\n",
    "    return any(c in TARGET_CATS for c in cat_list)\n",
    "\n",
    "df_base[\"has_target_cat\"] = df_base[\"categories_list\"].apply(has_target_cat)\n",
    "\n",
    "print(\"Total rows:\", len(df_base))\n",
    "print(\"Rows with target categories:\", df_base[\"has_target_cat\"].sum())\n",
    "print(\"Rows with abstract >= 200 chars:\", (df_base[\"abstract_len\"] >= 200).sum())\n",
    "\n",
    "df_rag = df_base[\n",
    "    (df_base[\"has_target_cat\"]) &\n",
    "    (df_base[\"abstract_len\"] >= MIN_ABSTRACT_CHARS)\n",
    "].copy()\n",
    "\n",
    "df_rag = df_rag.drop_duplicates(subset=[\"id\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"Filtered df_rag shape:\", df_rag.shape)\n",
    "df_rag.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc592ef2-2aea-400b-bd55-95ee9c7ed5a3",
   "metadata": {},
   "source": [
    "## Category filtering results (RAG corpus)\n",
    "\n",
    "- Loaded **10,000** arXiv records from the raw JSON snapshot.\n",
    "- Applied two filters to create a RAG-ready subset:\n",
    "  1. **Topic filter:** keep papers whose `categories` include at least one of  \n",
    "     `{\"cs.CL\", \"cs.LG\", \"cs.AI\"}`\n",
    "  2. **Quality filter:** keep papers with `abstract_len >= 200` characters\n",
    "\n",
    "### Summary\n",
    "- **Total rows:** 10,000  \n",
    "- **Rows with target categories:** 76  \n",
    "- **Rows with abstract ≥ 200 chars:** 9,659  \n",
    "- **Final filtered RAG dataset (`df_rag`) shape:** **(76, 12)**\n",
    "\n",
    "### Output dataset\n",
    "`df_rag` now contains the cleaned and filtered fields needed for RAG, including:\n",
    "- `id`, `title_clean`, `abstract_clean`\n",
    "- `categories`, `categories_list`\n",
    "- `update_date`, `authors_parsed`\n",
    "- `abstract_len`, `has_target_cat`\n",
    "\n",
    "This dataset will be used next to build the canonical document text (`doc_text`) and create retrieval chunks for embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e906acb2-32b0-415d-a7e8-920f8ed7597e",
   "metadata": {},
   "source": [
    "### Build doc_text (one document per paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "abfabc12-758c-4151-9e5e-18277eb7d362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title_clean</th>\n",
       "      <th>categories</th>\n",
       "      <th>abstract_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>704.0047</td>\n",
       "      <td>Intelligent location of simultaneously active acoustic emission sources: Part I</td>\n",
       "      <td>cs.NE cs.AI</td>\n",
       "      <td>1170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>704.0050</td>\n",
       "      <td>Intelligent location of simultaneously active acoustic emission sources: Part II</td>\n",
       "      <td>cs.NE cs.AI</td>\n",
       "      <td>948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>704.0304</td>\n",
       "      <td>The World as Evolving Information</td>\n",
       "      <td>cs.IT cs.AI math.IT q-bio.PE</td>\n",
       "      <td>788</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  \\\n",
       "0  704.0047   \n",
       "1  704.0050   \n",
       "2  704.0304   \n",
       "\n",
       "                                                                        title_clean  \\\n",
       "0   Intelligent location of simultaneously active acoustic emission sources: Part I   \n",
       "1  Intelligent location of simultaneously active acoustic emission sources: Part II   \n",
       "2                                                 The World as Evolving Information   \n",
       "\n",
       "                     categories  abstract_len  \n",
       "0                   cs.NE cs.AI          1170  \n",
       "1                   cs.NE cs.AI           948  \n",
       "2  cs.IT cs.AI math.IT q-bio.PE           788  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rag[\"update_date_parsed\"] = pd.to_datetime(df_rag[\"update_date\"], errors=\"coerce\")\n",
    "\n",
    "def build_doc_text(row):\n",
    "    cats = \" \".join(row[\"categories_list\"]) if isinstance(row[\"categories_list\"], list) else \"\"\n",
    "    date_part = \"\"\n",
    "    if pd.notna(row[\"update_date_parsed\"]):\n",
    "        date_part = f\"\\nLast updated: {row['update_date_parsed'].date()}\"\n",
    "    return (\n",
    "        f\"Title: {row['title_clean']}\\n\"\n",
    "        f\"Abstract: {row['abstract_clean']}\\n\"\n",
    "        f\"Categories: {cats}\"\n",
    "        f\"{date_part}\"\n",
    "    )\n",
    "\n",
    "df_rag[\"doc_text\"] = df_rag.apply(build_doc_text, axis=1)\n",
    "df_rag[[\"id\", \"title_clean\", \"categories\", \"abstract_len\"]].head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6436bec-3ade-4f91-aacb-c261f9f1a89c",
   "metadata": {},
   "source": [
    "## Build Canonical Document Text (`doc_text`)\n",
    "\n",
    "To prepare the dataset for Retrieval-Augmented Generation (RAG), we constructed a canonical text representation for each paper.\n",
    "\n",
    "Each `doc_text` includes:\n",
    "\n",
    "- **Title**\n",
    "- **Abstract**\n",
    "- **Categories**\n",
    "- **Last updated date (if available)**\n",
    "\n",
    "This ensures:\n",
    "- Consistent structure across documents\n",
    "- Rich contextual signals for embeddings\n",
    "- Better retrieval quality\n",
    "\n",
    "Example structure:\n",
    "\n",
    "Title: <paper title>  \n",
    "Abstract: <paper abstract>  \n",
    "Categories: <arXiv categories>  \n",
    "Last updated: <YYYY-MM-DD>\n",
    "\n",
    "The resulting dataset (`df_rag`) now contains one cleaned, structured document per paper, ready for chunking and embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4f5db8-ed6e-4799-a595-3904cd0dddee",
   "metadata": {},
   "source": [
    "### Chunk for Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f2d30182-725f-410e-8e1e-d0878561db43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents: 76\n",
      "Chunks: 136\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>chunk_index</th>\n",
       "      <th>chunk_text</th>\n",
       "      <th>title</th>\n",
       "      <th>categories</th>\n",
       "      <th>update_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>704.0047__0</td>\n",
       "      <td>704.0047</td>\n",
       "      <td>0</td>\n",
       "      <td>Title: Intelligent location of simultaneously active acoustic emission sources: Part I\\nAbstract: The intelligent ac...</td>\n",
       "      <td>Intelligent location of simultaneously active acoustic emission sources: Part I</td>\n",
       "      <td>cs.NE cs.AI</td>\n",
       "      <td>2009-09-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>704.0047__1</td>\n",
       "      <td>704.0047</td>\n",
       "      <td>1</td>\n",
       "      <td>rning from examples. Locator performance was tested on different test specimens. Tests have shown that the accuracy ...</td>\n",
       "      <td>Intelligent location of simultaneously active acoustic emission sources: Part I</td>\n",
       "      <td>cs.NE cs.AI</td>\n",
       "      <td>2009-09-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>704.005__0</td>\n",
       "      <td>704.0050</td>\n",
       "      <td>0</td>\n",
       "      <td>Title: Intelligent location of simultaneously active acoustic emission sources: Part II\\nAbstract: Part I describes ...</td>\n",
       "      <td>Intelligent location of simultaneously active acoustic emission sources: Part II</td>\n",
       "      <td>cs.NE cs.AI</td>\n",
       "      <td>2007-05-23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      chunk_id    doc_id  chunk_index  \\\n",
       "0  704.0047__0  704.0047            0   \n",
       "1  704.0047__1  704.0047            1   \n",
       "2   704.005__0  704.0050            0   \n",
       "\n",
       "                                                                                                                chunk_text  \\\n",
       "0  Title: Intelligent location of simultaneously active acoustic emission sources: Part I\\nAbstract: The intelligent ac...   \n",
       "1  rning from examples. Locator performance was tested on different test specimens. Tests have shown that the accuracy ...   \n",
       "2  Title: Intelligent location of simultaneously active acoustic emission sources: Part II\\nAbstract: Part I describes ...   \n",
       "\n",
       "                                                                              title  \\\n",
       "0   Intelligent location of simultaneously active acoustic emission sources: Part I   \n",
       "1   Intelligent location of simultaneously active acoustic emission sources: Part I   \n",
       "2  Intelligent location of simultaneously active acoustic emission sources: Part II   \n",
       "\n",
       "    categories update_date  \n",
       "0  cs.NE cs.AI  2009-09-29  \n",
       "1  cs.NE cs.AI  2009-09-29  \n",
       "2  cs.NE cs.AI  2007-05-23  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CHUNK_SIZE_CHARS = 800\n",
    "CHUNK_OVERLAP_CHARS = 150\n",
    "\n",
    "def chunk_text(text, chunk_size=CHUNK_SIZE_CHARS, overlap=CHUNK_OVERLAP_CHARS):\n",
    "    if not text:\n",
    "        return []\n",
    "    if chunk_size <= overlap:\n",
    "        raise ValueError(\"chunk_size must be > overlap\")\n",
    "\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    n = len(text)\n",
    "\n",
    "    while start < n:\n",
    "        end = min(start + chunk_size, n)\n",
    "        chunk = text[start:end].strip()\n",
    "        if chunk:\n",
    "            chunks.append(chunk)\n",
    "        if end == n:\n",
    "            break\n",
    "        start = end - overlap\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "rows = []\n",
    "for _, row in df_rag.iterrows():\n",
    "    doc_id = row[\"id\"]\n",
    "    chunks = chunk_text(row[\"doc_text\"])\n",
    "\n",
    "    for i, ch in enumerate(chunks):\n",
    "        rows.append({\n",
    "            \"chunk_id\": f\"{doc_id}__{i}\",\n",
    "            \"doc_id\": doc_id,\n",
    "            \"chunk_index\": i,\n",
    "            \"chunk_text\": ch,\n",
    "            \"title\": row[\"title_clean\"],\n",
    "            \"categories\": row[\"categories\"],\n",
    "            \"update_date\": row[\"update_date\"]\n",
    "        })\n",
    "\n",
    "df_chunks = pd.DataFrame(rows)\n",
    "\n",
    "print(\"Documents:\", len(df_rag))\n",
    "print(\"Chunks:\", len(df_chunks))\n",
    "df_chunks.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9da5439-adf8-410c-99a5-2206b10fea91",
   "metadata": {},
   "source": [
    "## Chunking Results\n",
    "\n",
    "The filtered RAG dataset contains:\n",
    "\n",
    "- **76 documents** (papers in cs.CL, cs.LG, cs.AI)\n",
    "- **136 retrieval chunks**\n",
    "\n",
    "Each document was split using:\n",
    "- Chunk size: 800 characters\n",
    "- Overlap: 150 characters\n",
    "\n",
    "Chunking ensures:\n",
    "- Better semantic retrieval\n",
    "- Improved answer grounding\n",
    "- Scalable architecture for larger corpora\n",
    "\n",
    "We now have:\n",
    "\n",
    "- `df_rag` → one row per paper  \n",
    "- `df_chunks` → one row per retrieval chunk  \n",
    "\n",
    "The chunked dataset is ready for embedding and vector indexing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06a602f-1c8a-46d1-98fc-8076056e6caf",
   "metadata": {},
   "source": [
    "## Save Clean Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b528ce48-51e0-4818-869e-6e620ceab02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved:\n",
      " - ..\\data\\processed\\arxiv_docs_clean_20260223_1021.csv\n",
      " - ..\\data\\processed\\arxiv_chunks_20260223_1021.csv\n",
      " - ..\\data\\processed\\schema_20260223_1021.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Save cleaned documents\n",
    "df_rag.to_csv(DOCS_OUT, index=False)\n",
    "\n",
    "# Save chunked corpus\n",
    "df_chunks.to_csv(CHUNKS_OUT, index=False)\n",
    "\n",
    "schema = {\n",
    "    \"documents\": len(df_rag),\n",
    "    \"chunks\": len(df_chunks),\n",
    "    \"target_categories\": sorted(list(TARGET_CATS)),\n",
    "    \"min_abstract_chars\": MIN_ABSTRACT_CHARS,\n",
    "    \"chunk_size_chars\": CHUNK_SIZE_CHARS,\n",
    "    \"chunk_overlap_chars\": CHUNK_OVERLAP_CHARS,\n",
    "    \"docs_path\": str(DOCS_OUT),\n",
    "    \"chunks_path\": str(CHUNKS_OUT)\n",
    "}\n",
    "\n",
    "with open(SCHEMA_OUT, \"w\") as f:\n",
    "    json.dump(schema, f, indent=2)\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\" -\", DOCS_OUT)\n",
    "print(\" -\", CHUNKS_OUT)\n",
    "print(\" -\", SCHEMA_OUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0dfedc-9a1b-4627-8e30-cd0cbcfc7493",
   "metadata": {},
   "source": [
    "## Data Preparation Complete\n",
    "\n",
    "The RAG corpus has been successfully prepared and saved.\n",
    "\n",
    "### Outputs Generated\n",
    "- Cleaned documents file  \n",
    "- Chunked retrieval corpus file  \n",
    "- Schema metadata JSON (for reproducibility)\n",
    "\n",
    "### Corpus Statistics\n",
    "- Documents: 76\n",
    "- Retrieval chunks: 136\n",
    "- Target domains: cs.CL, cs.LG, cs.AI\n",
    "\n",
    "This dataset is now ready for:\n",
    "\n",
    "1. Embedding generation\n",
    "2. Vector index construction\n",
    "3. Semantic retrieval testing\n",
    "4. RAG answer generation with citations\n",
    "\n",
    "The next step is to build the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f165d8-904d-47b3-a148-43c658799018",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
